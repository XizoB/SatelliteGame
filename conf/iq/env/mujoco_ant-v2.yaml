# @package _global_

env:
  name: Ant-v2
  demo: Ant-v2_25.pkl

  replay_mem: 1e6
  initial_mem: 1280

  horizon: 1000  # eps_steps/horizon 即智能体与环境交互的最长步骤
  eps_window: 10
  learn_steps: 5e5
  eval_interval: 5e3
  roullout_length : 1  # 每一次更新需要与环境交互采集的transition的数目
  train_steps: 1  # 每一次更新循环中，价值与策略网络的数目

expert:
  demos: 1
  subsample_freq: 1

eval:
  policy:  # 测试训练好的策略的文件所在位置 
  threshold: 4500

agent:
  name: sac
  init_temp: 0.001  # use a low temp for IL, 初始化自动调节正则化参数alpha

  actor_lr: 3e-4
  actor_update_frequency: 1  # 策略网络更新频率

  critic_lr: 3e-4
  critic_target_update_frequency: 1  # 目标价值网络更新频率

  # learn temperature coefficient (disabled by default) 学习温度系数
  learn_temp: False

log_interval: 500  # Log every this many steps
num_actor_updates: 1

train:
  use_target: true
  soft_update: true
  batch: 256

q_net:
  _target_: agent.sac_models.SingleQCritic