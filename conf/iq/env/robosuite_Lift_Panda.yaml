# @package _global_

env:
  name: robosuite_Lift_Panda
  task: Lift
  robots: Panda
  demo: # IL 模仿学习

  default_controller: OSC_POSE
  control_freq: 20
  reward_shaping: True
  reward_scale: 1.0
  use_camera_obs: False
  ignore_done: True
  hard_reset: False
  has_renderer: False
  has_offscreen_renderer: False

  replay_mem: 1000000
  initial_mem: 5000
  # initial_mem: 3300

  horizon: 500  # eps_steps/horizon 即智能体与环境交互的最长步骤
  eps_window: 10
  learn_steps: 5e5  # 智能体的学习步数
  eval_interval: 1000  # 评估智能体的步数
  roullout_length : 2500  # 每一次更新需要与环境交互采集的transition的数目
  train_steps: 1000  # 每一次更新循环中，价值与策略网络的数目


expert:
  demos: 1
  subsample_freq: 1

eval:
  policy: 
  eps: 10
  threshold: 4500

agent:
  name: sac
  init_temp: 0.01 # use a low temp for IL, 初始化自动调节正则化参数alpha

  actor_lr: 0.0003
  actor_update_frequency: 1  # 策略网络更新频率

  critic_lr: 0.0003
  critic_target_update_frequency: 1  # 目标价值网络更新频率

  # learn temperature coefficient (disabled by default) 学习温度系数
  learn_temp: True

num_seed_steps: 0 # Don't need seeding for IL (Use 1000 for RL)
log_interval: 500  # Log every this many steps
num_actor_updates: 1  # 模仿学习中的参数 策略更新次数 

train:
  use_target: true
  soft_update: true
  batch: 128

q_net:
  _target_: agent.sac_models.SingleQCritic